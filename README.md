# Spam-Ham-classification

There are three datasets. Each data set is divided into two (sub)sets: training set and test set. Each of them has two directories: spam and ham. All files in the spam and ham folders are spam and ham messages respectively.

Convert the text data into a matrix of features × examples (namely our canonical data representation), using the following approaches.

**Bag of Words model** : we use a vocabulary having w words—the set of all unique words in the training set—and represent each email using a vector of word frequencies (the number of times each word in the vocabulary appears in the email).

**Bernoulli model** : we use a vocabulary having w words and represent each email (training example) using a 0/1 vector of length w where 0 indicates that the word does not appear in the email and 1 indicates that the word appears in the email.
Thus you will convert each of the three “text” datasets into two datasets, one using the Bag of words model and the other using the Bernoulli model.

**Multinomial Naive Bayes** : Implemented the multinomial Naive Bayes algorithm for text classification that uses add-one laplace smoothing using dataset generated by Bag of Words model.

**Discrete Naive Bayes** : Implemented the discrete Naive Bayes algorithm for text classification that uses add-one laplace smoothing using dataset generated by Bernoulli model.

**MCAP Logistic Regression** : Implemented the MCAP Logistic Regression algorithm on the datasets generated by both Bag of Words and Bernoulli model with L2 regularization. Tried different values of λ, divided the given training set into two sets using a 70/30 split (namely the first split has 70% of the examples and the second split has the remaining 30%). Learnt parameters using the 70% split, treated the 30% data as validation data and use it to select a value for λ. Then, used the chosen value of λ to learn the parameters using the full training set and report accuracy on the test set. Used gradient ascent for learning the weights (you have to set the learning rate appropriately and put a suitable hard limit on the number of iterations.

**SGDClassifier** : Implemented the SGDClassifier from scikit-learn on the datasets generated by both Bag of Words and Bernoulli model. Tuned the parameters (e.g., loss function, penalty, etc.) of the SGDClassifier using GridSearchCV in scikit-learn.

